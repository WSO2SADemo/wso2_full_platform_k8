jobs:
  build-and-deploy:
    runs-on: self-hosted

    env:
      BALLERINA_NAMESPACE: ballerina 
      PROJECT_DIR: apim-integration-artefacts-for-cicd-pipeline/integration/integrations-2/mock_backends

    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: Print Ballerina Version
        run: bal version

      - name: Build Ballerina Project
        working-directory: ${{ env.PROJECT_DIR }}
        run: bal build
      
      - name: üîê Docker Login
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
          
      - name: Push Docker Image
        run: |
          # 1. Find the image ID built by Ballerina (uses the most recent one)
          BUILT_IMAGE=$(docker images --format "{{.Repository}}:{{.Tag}}" | grep "ramilu90/mock_backend" | head -n 1)
          echo "Found built image: $BUILT_IMAGE"
          
          # 2. Retag explicitly to match your deployment
          # (Ensure this tag matches what is in your K8s YAML)
          TARGET_IMAGE="ramilu90/mock_backend:1.0.5"
          docker tag "$BUILT_IMAGE" "$TARGET_IMAGE"
          
          # 3. Push
          docker push "$TARGET_IMAGE"

      - name: Ensure Namespace Exists
        run: kubectl create namespace ${{ env.BALLERINA_NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -

      # -----------------------------------------------------------
      # ‚úÖ FIX 1: ConfigMap with CORRECT formatting (using | )
      # -----------------------------------------------------------
      - name: Create ConfigMap
        run: |
          NAMESPACE="${{ env.BALLERINA_NAMESPACE }}"
          
          # We use cat <<EOF to handle newlines correctly. 
          # DO NOT use single quotes for the data block.
          cat <<EOF > configmap.yaml
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: ballerina-values-mock-backends
            namespace: $NAMESPACE
          data:
            BAL_CONFIG_VAR_BALLERINAX_WSO2_CONTROLPLANE_NODEID: "mock_backends"
            BAL_CONFIG_DATA: |
              [ballerinax.wso2.controlplane.dashboard]
              url="https://cloud-icp.icp.svc.cluster.local:9743/dashboard/api"
              heartbeatInterval=10
              groupId="cluster1"
              mgtApiUrl="https://mock-backends.ballerina.svc.cluster.local:9264/management/"
              nodeId="mock_backends"
          EOF

          echo "Applying Corrected ConfigMap..."
          kubectl apply -f configmap.yaml

      - name: Deploy Kubernetes Resources
        run: |
          K8S_DIR="${{ env.PROJECT_DIR }}/target/kubernetes/mock_backends"
          echo "Applying resources to Local Cluster..."
          kubectl apply -f ${K8S_DIR} -n ${{ env.BALLERINA_NAMESPACE }}

      # -----------------------------------------------------------
      # ‚úÖ FIX 2: Robust Service & Patching Logic
      # -----------------------------------------------------------
      - name: Ensure Service & Patch Ports
        run: |
          NAMESPACE="${{ env.BALLERINA_NAMESPACE }}"
          DEPLOYMENT="mock-backends-deployment"
          SERVICE_NAME="mock-backends" 

          # 1. Create Service if missing
          if ! kubectl get svc $SERVICE_NAME -n $NAMESPACE > /dev/null 2>&1; then
             echo "Service not found. Creating it now..."
             kubectl expose deployment $DEPLOYMENT -n $NAMESPACE --name=$SERVICE_NAME --port=9092 --target-port=9092 --type=ClusterIP
          else
             echo "Service $SERVICE_NAME already exists."
          fi

          # 2. PATCH: Force add port 9264 if not present
          # We check if port 9264 is already in the list to avoid duplicate errors
          if kubectl get svc $SERVICE_NAME -n $NAMESPACE -o jsonpath='{.spec.ports[*].port}' | grep -q "9264"; then
             echo "Port 9264 already exists. Skipping patch."
          else
             echo "Port 9264 missing. Patching service..."
             kubectl patch svc $SERVICE_NAME -n $NAMESPACE --type='json' -p='[{"op": "add", "path": "/spec/ports/-", "value": {"name": "management", "port": 9264, "targetPort": 9264, "protocol": "TCP"}}]'
          fi

      - name: Patch Deployment (Volume Mounts & Env)
        run: |
          NAMESPACE="${{ env.BALLERINA_NAMESPACE }}"
          DEPLOYMENT="mock-backends-deployment"
          
          # Note: We do NOT wait for rollout here because if the pod is crashing, 
          # the rollout hangs and fails. We patch immediately to fix the crash.
          
          CONTAINER_NAME=$(kubectl get deployment $DEPLOYMENT -n $NAMESPACE -o jsonpath='{.spec.template.spec.containers[0].name}')
          
          echo "Patching Deployment $DEPLOYMENT (Container: $CONTAINER_NAME)..."
          
          kubectl patch deployment $DEPLOYMENT -n $NAMESPACE --patch "
          spec:
            template:
              spec:
                volumes:
                - name: keystore-vol
                  secret:
                    secretName: ballerina-integration-secret
                containers:
                - name: $CONTAINER_NAME
                  envFrom:
                  - configMapRef:
                      name: ballerina-values-mock-backends
                  volumeMounts:
                  - name: keystore-vol
                    mountPath: /home/ballerina/bre/security
                    readOnly: true
          "
          
          # Restart to ensure new ConfigMap and Volume mounts are picked up
          kubectl rollout restart deployment/$DEPLOYMENT -n $NAMESPACE
          
          echo "Waiting for successful rollout..."
          kubectl rollout status deployment/$DEPLOYMENT -n $NAMESPACE --timeout=120s